{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ம ஹா ம்ரி த் யு ஞ் ஜ யா\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\nம ஹா ம்ரி த் யு ஞ் ஜ யா\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Box_filter(values, kernel_size=3):\n",
    "    filter_values = np.cumsum(values, dtype=float)\n",
    "\n",
    "    filter_values[kernel_size:] = filter_values[kernel_size:] - filter_values[:-kernel_size]\n",
    "    filter_values[kernel_size:] = filter_values[kernel_size:] / kernel_size\n",
    "    for i in range(1, kernel_size):\n",
    "        filter_values[i] /= i + 1\n",
    "    return filter_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrapolate_next(values):\n",
    "    last_value = values[-1]\n",
    "    slope = [(last_value - v) / i for (i, v) in enumerate(values[::-1])]\n",
    "    slope[0] = 0\n",
    "    next_values = last_value + np.cumsum(slope)\n",
    "\n",
    "    return next_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_series(values, extend_num=5, forward=5):\n",
    "\n",
    "    next_value = extrapolate_next(values)[forward]\n",
    "    extension = [next_value] * extend_num\n",
    "\n",
    "    if isinstance(values, list):\n",
    "        merge_values = values + extension\n",
    "    else:\n",
    "        merge_values = np.append(values, extension)\n",
    "    return merge_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Saliency(object):\n",
    "    def __init__(self, amp_window_size, series_window_size, score_window_size):\n",
    "        self.amp_window_size = amp_window_size\n",
    "        self.series_window_size = series_window_size\n",
    "        self.score_window_size = score_window_size\n",
    "\n",
    "    def frequency_domain_map(self, values):\n",
    "        freq = np.fft.fft(values)\n",
    "        mag = np.sqrt(freq.real ** 2 + freq.imag ** 2)\n",
    "        spectral_residual = np.exp(np.log(mag) - Box_filter(np.log(mag), self.amp_window_size))\n",
    "        freq.real = freq.real * spectral_residual / mag\n",
    "        freq.imag = freq.imag * spectral_residual / mag\n",
    "        freqmap = np.fft.ifft(freq)\n",
    "        freq_map = np.sqrt(freqmap.real ** 2 + freqmap.imag ** 2)\n",
    "        return freq_map\n",
    "\n",
    "    def generate_anomaly_score(self, values, type=\"avg\"):\n",
    "        extended_series = merge_series(values, self.series_window_size, self.series_window_size)\n",
    "        mag = self.frequency_domain_map(extended_series)[: len(values)]\n",
    "        ave_filter = Box_filter(mag, self.score_window_size)        \n",
    "        score = (mag - ave_filter) / ave_filter \n",
    "        return score\n",
    "\n",
    "    def generate_anomaly_score2(self, values, type=\"avg\"):\n",
    "        extended_series = merge_series(values, self.series_window_size, self.series_window_size)\n",
    "        mag = self.frequency_domain_map(extended_series)[: len(values)]\n",
    "        return mag\n",
    "\n",
    "    def generate_anomaly_score_(self, values, type=\"avg\"):\n",
    "        extended_series = merge_series(values, self.series_window_size, self.series_window_size)\n",
    "        mag = self.frequency_domain_map(extended_series)[: len(values)]\n",
    "        ave_filter = Box_filter(mag, self.score_window_size)\n",
    "        ave_filter1 = Box_filter(ave_filter, 12)\n",
    "        ave_filter2 = Box_filter(ave_filter1, 8)\n",
    "        ave_filter = Box_filter(ave_filter2, 6)\n",
    "        score = (mag - ave_filter) / ave_filter        \n",
    "        return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entire A1, A2, A3, A4 benchmark Yahoo opensource data in single folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/manjunath.adinarayan/TIME-SERIES/YAHOO_Data/Full_data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "367"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "extension='csv'\n",
    "os.chdir( path )\n",
    "files = glob.glob('*.{}'.format(extension))\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# less than period\n",
    "amp_window_size=3\n",
    "# (maybe) as same as period\n",
    "series_window_size=3\n",
    "# a number enough larger than period\n",
    "score_window_size=32\n",
    "\n",
    "spec = Saliency(amp_window_size, series_window_size, score_window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESR-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTIPLE FILTER LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FULL- DATA [ Cold-start ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6972874391398184, 0.806273208068566, 0.6936235496074371)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prec_list, rec_list, f1score_list = [], [], []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(os.path.join(path,file), header=0)\n",
    "    test_signal = df.value.values \n",
    "    score = spec.generate_anomaly_score_(test_signal)\n",
    "#    index_changes = np.where(score > np.percentile(score, 99))[0]\n",
    "\n",
    "    cutoff = 4.8\n",
    "    pred_anom = []\n",
    "    for i in range(0, len(score)):\n",
    "        if score[i] > cutoff:\n",
    "            pred_anom.append(1)\n",
    "        else:\n",
    "            pred_anom.append(0)\n",
    "            \n",
    "    if \"Benchmark\" in file:\n",
    "        prec_list.append(precision_score(df.anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.anomaly.values, pred_anom) )\n",
    "    else:\n",
    "        prec_list.append(precision_score(df.is_anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.is_anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.is_anomaly.values, pred_anom) )\n",
    "\n",
    "np.mean(prec_list), np.mean(rec_list), np.mean(f1score_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECOND HALF OF DATA [ TEST DATA ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7137222053382496, 0.7905652822820916, 0.6956508794793295)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_list, rec_list, f1score_list = [], [], []\n",
    "\n",
    "for file in files:\n",
    "    df1 = pd.read_csv(os.path.join(path,file), header=0)\n",
    "    df = df1.iloc[int(len(df1)/2):, :]\n",
    "    test_signal = df.value.values \n",
    "    score = spec.generate_anomaly_score_(test_signal)\n",
    "    #index_changes = np.where(score > np.percentile(score, 99))[0]\n",
    "\n",
    "    cutoff = 4.8\n",
    "    pred_anom = []\n",
    "    for i in range(0, len(score)):\n",
    "        if score[i] > cutoff:\n",
    "            pred_anom.append(1)\n",
    "        else:\n",
    "            pred_anom.append(0)\n",
    "            \n",
    "    if \"Benchmark\" in file:\n",
    "        prec_list.append(precision_score(df.anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.anomaly.values, pred_anom) )\n",
    "    else:\n",
    "        prec_list.append(precision_score(df.is_anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.is_anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.is_anomaly.values, pred_anom) )\n",
    "\n",
    "np.mean(prec_list), np.mean(rec_list), np.mean(f1score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESR-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced SR with no averaging/filtering of saliency map and cutoff = 7 * np.mean(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FULL- DATA [ Cold-start ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6509540021083907, 0.7779652423245614, 0.6508797391830556)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_list, rec_list, f1score_list = [], [], []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(os.path.join(path,file), header=0)\n",
    "    test_signal = df.value.values \n",
    "    score = spec.generate_anomaly_score2(test_signal)\n",
    "    #index_changes = np.where(score > np.percentile(score, 99))[0]\n",
    "    \n",
    "    cutoff = 7 * np.mean(score)\n",
    "    pred_anom = []\n",
    "    for i in range(0, len(score)):\n",
    "        if score[i] > cutoff:\n",
    "            pred_anom.append(1)\n",
    "        else:\n",
    "            pred_anom.append(0)\n",
    "            \n",
    "    if \"Benchmark\" in file:\n",
    "        prec_list.append(precision_score(df.anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.anomaly.values, pred_anom) )\n",
    "    else:\n",
    "        prec_list.append(precision_score(df.is_anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.is_anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.is_anomaly.values, pred_anom) )\n",
    "\n",
    "np.mean(prec_list), np.mean(rec_list), np.mean(f1score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HALF OF THE DATA [ TEST DATA ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.675088970218398, 0.7560466291248261, 0.6570270138255867)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_list, rec_list, f1score_list = [], [], []\n",
    "\n",
    "for file in files:\n",
    "    df1 = pd.read_csv(os.path.join(path,file), header=0)\n",
    "    df = df1.iloc[int(len(df1)/2):, :]\n",
    "    test_signal = df.value.values \n",
    "    score = spec.generate_anomaly_score2(test_signal)\n",
    "    #index_changes = np.where(score > np.percentile(score, 99))[0]\n",
    "    \n",
    "    cutoff = 7 * np.mean(score)\n",
    "    pred_anom = []\n",
    "    for i in range(0, len(score)):\n",
    "        if score[i] > cutoff:\n",
    "            pred_anom.append(1)\n",
    "        else:\n",
    "            pred_anom.append(0)\n",
    "            \n",
    "    if \"Benchmark\" in file:\n",
    "        prec_list.append(precision_score(df.anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.anomaly.values, pred_anom) )\n",
    "    else:\n",
    "        prec_list.append(precision_score(df.is_anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.is_anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.is_anomaly.values, pred_anom) )\n",
    "\n",
    "np.mean(prec_list), np.mean(rec_list), np.mean(f1score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESR-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced SR with averaging/filtering of saliency map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # FULL- DATA [ Cold-start ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5968055594659185, 0.8690790440124405, 0.6590783629749551)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_list, rec_list, f1score_list = [], [], []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(os.path.join(path,file), header=0)\n",
    "    test_signal = df.value.values \n",
    "    score = spec.generate_anomaly_score(test_signal)\n",
    "    #index_changes = np.where(score > np.percentile(score, 99))[0]\n",
    "    cutoff = 2.8\n",
    "    pred_anom = []\n",
    "    for i in range(0, len(score)):\n",
    "        if score[i] > cutoff:\n",
    "            pred_anom.append(1)\n",
    "        else:\n",
    "            pred_anom.append(0)\n",
    "            \n",
    "    if \"Benchmark\" in file:\n",
    "        prec_list.append(precision_score(df.anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.anomaly.values, pred_anom) )\n",
    "    else:\n",
    "        prec_list.append(precision_score(df.is_anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.is_anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.is_anomaly.values, pred_anom) )\n",
    "\n",
    "np.mean(prec_list), np.mean(rec_list), np.mean(f1score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HALF OF THE DATA [ TEST DATA ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/manjunath.adinarayan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6589798720628981, 0.8312036686714798, 0.6940316418644611)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prec_list, rec_list, f1score_list = [], [], []\n",
    "\n",
    "for file in files:\n",
    "    df1 = pd.read_csv(os.path.join(path,file), header=0)\n",
    "    df = df1.iloc[int(len(df1)/2):, :]\n",
    "    test_signal = df.value.values \n",
    "    score = spec.generate_anomaly_score(test_signal)\n",
    "    #index_changes = np.where(score > np.percentile(score, 99))[0]\n",
    "    cutoff = 2.8\n",
    "    pred_anom = []\n",
    "    for i in range(0, len(score)):\n",
    "        if score[i] > cutoff:\n",
    "            pred_anom.append(1)\n",
    "        else:\n",
    "            pred_anom.append(0)\n",
    "            \n",
    "    if \"Benchmark\" in file:\n",
    "        prec_list.append(precision_score(df.anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.anomaly.values, pred_anom) )\n",
    "    else:\n",
    "        prec_list.append(precision_score(df.is_anomaly.values, pred_anom) )\n",
    "        rec_list.append(recall_score(df.is_anomaly.values, pred_anom) )\n",
    "        f1score_list.append(f1_score(df.is_anomaly.values, pred_anom) )\n",
    "\n",
    "np.mean(prec_list), np.mean(rec_list), np.mean(f1score_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
